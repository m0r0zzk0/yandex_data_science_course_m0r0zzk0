# Мини проект по bias-variance tradeoff

## Цель проекта: показать на синтетических данных (в данном случае использовался график функции синуса), что должен существовать некоторый баланс между ошибкой и сложностью модели.

## Выводы:

Линейная регрессия (High Bias)
Низкая сложность модели - не может аппроксимировать нелинейную функцию

Высокий bias - систематическая ошибка из-за слишком простой модели

Низкая variance - предсказания стабильны на разных наборах данных

Признаки недообучения: высокая ошибка на train и test данных

2. Полиномиальная регрессия степени 3 (Balanced)
Оптимальная сложность - хорошо балансирует bias и variance

Хорошо аппроксимирует истинную функцию sin(2πx)

Стабильная производительность на train и test данных

Минимальный разрыв между train и test ошибкой

3. Полиномиальная регрессия степени 15 (High Variance)
Слишком высокая сложность - подстраивается под шум

Низкий bias - хорошо обучается на тренировочных данных

Высокая variance - большая разница в предсказаниях на разных данных

Признаки переобучения: низкая train ошибка, но высокая test ошибка

## Практические инсайты для портфолио:

Bias-Variance Tradeoff - фундаментальный компромисс в ML

Сложность модели должна соответствовать сложности данных

Кривые обучения помогают диагностировать проблемы

Регуляризация может помочь сбалансировать bias и variance
